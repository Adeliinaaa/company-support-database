# –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–æ–≤ –∫–æ–º–ø–∞–Ω–∏–π –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤ 10+ —á–µ–ª–æ–≤–µ–∫

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
from typing import Dict, List, Optional
import urllib.parse
from urllib.parse import urljoin
import os


def load_companies_from_csv(filename: str = None) -> pd.DataFrame:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–º–ø–∞–Ω–∏–π –∏–∑ CSV —Ñ–∞–π–ª–∞
    """
    if filename is None:
        # –ò—â–µ–º —Ñ–∞–π–ª –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
        possible_paths = [
            'data/raw/companies_seed.csv',
            '../data/raw/companies_seed.csv',
            os.path.join(os.path.dirname(__file__), '../data/raw/companies_seed.csv'),
            os.path.join(os.path.expanduser("~"), "Desktop", "companies_seed.csv")
        ]

        for path in possible_paths:
            if os.path.exists(path):
                filename = path
                break

    if not filename or not os.path.exists(filename):
        raise FileNotFoundError(f"–§–∞–π–ª —Å –∫–æ–º–ø–∞–Ω–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–∫–∞–ª–∏: {possible_paths}")

    print(f"üìÅ –ó–∞–≥—Ä—É–∂–∞—é –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑: {filename}")
    df = pd.read_csv(filename, encoding='utf-8-sig')
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∫–æ–º–ø–∞–Ω–∏–π")
    return df


def normalize_url(url: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL
    """
    if not url:
        return ""

    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url

    return url.rstrip('/')


def safe_request(url: str, max_retries: int = 2) -> Optional[requests.Response]:
    """
    –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ —Å–∞–π—Ç—É —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
    }

    for attempt in range(max_retries):
        try:
            response = requests.get(url, headers=headers, timeout=15, verify=False)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            print(f"   ‚ö†Ô∏è  –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries} –¥–ª—è {url}: {e}")
            if attempt < max_retries - 1:
                time.sleep(2)
            continue
        except Exception as e:
            print(f"   ‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –¥–ª—è {url}: {e}")
            break

    return None


def find_support_features(html: str, url: str, company_name: str) -> Dict:
    """
    –ò—â–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
    """
    soup = BeautifulSoup(html, 'html.parser')
    text_lower = soup.get_text().lower()

    # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    features = {
        'has_support_email': False,
        'has_contact_form': False,
        'has_online_chat': False,
        'has_messengers': False,
        'has_support_section': False,
        'has_kb_or_faq': False,
        'mentions_24_7': False,
        'support_team_size_min': 0,
        'support_evidence': '',
        'evidence_url': '',
        'evidence_type': 'site',
        'support_email': '',
        'support_url': '',
        'kb_url': '',
        'chat_vendor': '',
        'source': 'company_site'
    }

    # 1. –ü–æ–∏—Å–∫ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ 24/7 (–£—Ä–æ–≤–µ–Ω—å B)
    patterns_24_7 = [
        r'24/7', r'24\s*—á–∞—Å–∞', r'–∫—Ä—É–≥–ª–æ—Å—É—Ç–æ—á–Ω–æ', r'–∫—Ä—É–≥–ª—ã–µ\s*—Å—É—Ç–∫–∏',
        r'–≤—Å–µ–≥–¥–∞\s*–Ω–∞\s*—Å–≤—è–∑–∏', r'—Ä–∞–±–æ—Ç–∞–µ–º\s*–±–µ–∑\s*–≤—ã—Ö–æ–¥–Ω—ã—Ö',
        r'–ø–æ–¥–¥–µ—Ä–∂–∫–∞\s*24', r'24\s*—á–∞—Å–∞\s*–≤\s*—Å—É—Ç–∫–∏', r'24x7'
    ]

    for pattern in patterns_24_7:
        if re.search(pattern, text_lower, re.IGNORECASE):
            features['mentions_24_7'] = True
            # –ï—Å–ª–∏ –µ—Å—Ç—å 24/7, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –º–∏–Ω–∏–º—É–º 10 —á–µ–ª–æ–≤–µ–∫ (—Å–º–µ–Ω–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫)
            features['support_team_size_min'] = 10
            features['support_evidence'] = f"–ü–æ–¥–¥–µ—Ä–∂–∫–∞ 24/7 (—Å–º–µ–Ω–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ —Ç—Ä–µ–±—É–µ—Ç –º–∏–Ω–∏–º—É–º 10 —á–µ–ª–æ–≤–µ–∫)"
            features['evidence_url'] = url
            break

    # 2. –ü–æ–∏—Å–∫ email –ø–æ–¥–¥–µ—Ä–∂–∫–∏
    email_patterns = [
        r'support@[\w\.-]+', r'help@[\w\.-]+', r'info@[\w\.-]+',
        r'–ø–æ–¥–¥–µ—Ä–∂–∫–∞@[\w\.-]+', r'–ø–æ–º–æ—â—å@[\w\.-]+'
    ]

    for pattern in email_patterns:
        emails = re.findall(pattern, text_lower)
        for email in emails:
            features['has_support_email'] = True
            features['support_email'] = email
            break

    # 3. –ü–æ–∏—Å–∫ –∫–æ–Ω—Ç–∞–∫—Ç–Ω–æ–π —Ñ–æ—Ä–º—ã
    form_keywords = ['—Ñ–æ—Ä–º–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏', '–∫–æ–Ω—Ç–∞–∫—Ç–Ω–∞—è —Ñ–æ—Ä–º–∞', '–Ω–∞–ø–∏—à–∏—Ç–µ –Ω–∞–º',
                     '–æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å', '–∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å', 'contact form']

    forms = soup.find_all(['form', 'div', 'section'])
    for form in forms:
        form_text = form.get_text().lower()
        if any(keyword in form_text for keyword in form_keywords):
            features['has_contact_form'] = True
            break

    # 4. –ü–æ–∏—Å–∫ –æ–Ω–ª–∞–π–Ω-—á–∞—Ç–∞
    chat_indicators = ['—á–∞—Ç', 'online chat', 'live chat', '–æ–Ω–ª–∞–π–Ω-—á–∞—Ç',
                       'jivo', 'livechat', 'chatra', 'drift']

    for indicator in chat_indicators:
        if indicator in text_lower:
            features['has_online_chat'] = True
            break

    # 5. –ü–æ–∏—Å–∫ –º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä–æ–≤
    messengers = ['telegram', 'whatsapp', 'viber', 'vkontakte', 'vk.com']
    for messenger in messengers:
        if messenger in text_lower:
            features['has_messengers'] = True
            break

    # 6. –ü–æ–∏—Å–∫ —Ä–∞–∑–¥–µ–ª–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏
    support_keywords = ['–ø–æ–¥–¥–µ—Ä–∂–∫', '–ø–æ–º–æ—â—å', 'help', 'support', '—Å–ª—É–∂–±–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏',
                        '–∫–æ–Ω—Ç–∞–∫—Ç-—Ü–µ–Ω—Ç—Ä', '—Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞', 'customer support']

    links = soup.find_all('a')
    for link in links:
        link_text = link.get_text().lower()
        link_href = link.get('href', '')

        if any(keyword in link_text for keyword in support_keywords):
            features['has_support_section'] = True
            features['support_url'] = urljoin(url, link_href)
            break

    # 7. –ü–æ–∏—Å–∫ FAQ / –ë–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    faq_keywords = ['faq', '—á–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ', '–≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã',
                    '–±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π', 'knowledge base', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏']

    for link in links:
        link_text = link.get_text().lower()
        link_href = link.get('href', '')

        if any(keyword in link_text for keyword in faq_keywords):
            features['has_kb_or_faq'] = True
            features['kb_url'] = urljoin(url, link_href)
            break

    # 8. –ü–û–ò–°–ö –î–û–ö–ê–ó–ê–¢–ï–õ–¨–°–¢–í 10+ –ß–ï–õ–û–í–ï–ö (–°–ê–ú–û–ï –í–ê–ñ–ù–û–ï!)
    # –£—Ä–æ–≤–µ–Ω—å A: –ø—Ä—è–º–æ–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ —á–∏—Å–ª–∞
    size_patterns = [
        r'(\d+)\s*—Å–æ—Ç—Ä—É–¥–Ω–∏–∫[–∞-—è]*\s*–ø–æ–¥–¥–µ—Ä–∂–∫',
        r'(\d+)\s*—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç[–∞-—è]*\s*–ø–æ–¥–¥–µ—Ä–∂–∫',
        r'(\d+)\s*–æ–ø–µ—Ä–∞—Ç–æ—Ä[–∞-—è]*\s*–ø–æ–¥–¥–µ—Ä–∂–∫',
        r'–ø–æ–¥–¥–µ—Ä–∂–∫–∞\s*–∏–∑\s*(\d+)\s*—á–µ–ª–æ–≤–µ–∫',
        r'–∫–æ–Ω—Ç–∞–∫—Ç-—Ü–µ–Ω—Ç—Ä\s*(\d+)\s*–æ–ø–µ—Ä–∞—Ç–æ—Ä',
        r'(\d+)\s*—á–µ–ª–æ–≤–µ–∫\s*–≤\s*–ø–æ–¥–¥–µ—Ä–∂–∫–µ',
        r'–∫–æ–º–∞–Ω–¥–∞\s*–ø–æ–¥–¥–µ—Ä–∂–∫–∏\s*–≤\s*(\d+)\s*—á–µ–ª–æ–≤–µ–∫'
    ]

    for pattern in size_patterns:
        match = re.search(pattern, text_lower, re.IGNORECASE)
        if match:
            try:
                team_size = int(match.group(1))
                if team_size >= 10:
                    # –ù–∞—à–ª–∏ –ø—Ä—è–º–æ–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ (–£—Ä–æ–≤–µ–Ω—å A)
                    features['support_team_size_min'] = team_size
                    features['support_evidence'] = f"–ü—Ä—è–º–æ–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –Ω–∞ —Å–∞–π—Ç–µ: '{match.group(0)}'"
                    features['evidence_url'] = url
                    break
            except:
                continue

    return features


def analyze_company_website(company: Dict) -> Dict:
    """
    –ê–Ω–∞–ª–∏–∑ —Å–∞–π—Ç–∞ –æ–¥–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏
    """
    company_name = company.get('name', 'Unknown')
    site_url = company.get('site_url', '')

    print(f"\nüîç –ê–Ω–∞–ª–∏–∑: {company_name}")
    print(f"   URL: {site_url}")

    if not site_url or pd.isna(site_url):
        print("   ‚ö†Ô∏è  –ù–µ—Ç URL –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return company

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º URL
    site_url = normalize_url(site_url)

    # –î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å
    response = safe_request(site_url)

    if not response:
        print("   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–∞–π—Ç")
        return company

    # –ò—â–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏
    features = find_support_features(response.text, site_url, company_name)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏ —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    result = {**company, **features}

    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    found_features = []
    if features['has_support_email']: found_features.append("email")
    if features['has_contact_form']: found_features.append("—Ñ–æ—Ä–º–∞")
    if features['has_online_chat']: found_features.append("—á–∞—Ç")
    if features['has_support_section']: found_features.append("—Ä–∞–∑–¥–µ–ª")
    if features['has_kb_or_faq']: found_features.append("FAQ")
    if features['mentions_24_7']: found_features.append("24/7")

    if found_features:
        print(f"   ‚úÖ –ü—Ä–∏–∑–Ω–∞–∫–∏: {', '.join(found_features)}")
    else:
        print(f"   ‚ö†Ô∏è  –ü—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")

    if features['support_team_size_min'] >= 10:
        print(f"   üéØ –î–û–ö–ê–ó–ê–¢–ï–õ–¨–°–¢–í–û 10+: {features['support_team_size_min']} —á–µ–ª–æ–≤–µ–∫")

    # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å —Å–∞–π—Ç—ã
    time.sleep(1)

    return result


def save_enriched_data(enriched_df: pd.DataFrame, filename: str = 'data/raw/enriched_companies.csv'):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    """
    import os

    os.makedirs(os.path.dirname(filename), exist_ok=True)

    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω—è—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤: {filename}")
    print(f"üìä –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {len(enriched_df)} —Å—Ç—Ä–æ–∫, {len(enriched_df.columns)} –∫–æ–ª–æ–Ω–æ–∫")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –∫–æ–º–ø–∞–Ω–∏–∏
    print("\nüìã –ü–µ—Ä–≤—ã–µ 3 –∑–∞–ø–∏—Å–∏:")
    for i, row in enriched_df.head(3).iterrows():
        print(f"  {i + 1}. {row['name']} - –ø–æ–¥–¥–µ—Ä–∂–∫–∞: {row.get('support_team_size_min', 0)} —á–µ–ª.")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    enriched_df.to_csv(filename, index=False, encoding='utf-8-sig')

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω
    if os.path.exists(filename):
        file_size = os.path.getsize(filename)
        print(f"‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ! –†–∞–∑–º–µ—Ä: {file_size} –±–∞–π—Ç")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        with open(filename, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            print(f"üìÑ –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –≤ —Ñ–∞–π–ª–µ: {len(lines)}")

            if len(lines) > 1:
                print(f"üìã –ó–∞–≥–æ–ª–æ–≤–∫–∏: {lines[0].strip()}")
                print(f"üìù –ü–µ—Ä–≤–∞—è –∑–∞–ø–∏—Å—å: {lines[1].strip()[:100]}...")
            else:
                print("‚ö†Ô∏è  –í —Ñ–∞–π–ª–µ —Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏!")
    else:
        print("‚ùå –û–®–ò–ë–ö–ê: —Ñ–∞–π–ª –Ω–µ —Å–æ–∑–¥–∞–Ω!")

    return filename


def process_companies(companies_df: pd.DataFrame, limit: int = 20) -> pd.DataFrame:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ –∫–æ–º–ø–∞–Ω–∏–π
    """
    print(f"\n{'=' * 60}")
    print("–ê–ù–ê–õ–ò–ó –°–ê–ô–¢–û–í –ö–û–ú–ü–ê–ù–ò–ô –ù–ê –ü–†–ò–ó–ù–ê–ö–ò –ü–û–î–î–ï–†–ñ–ö–ò")
    print(f"{'=' * 60}")

    results = []

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Ç–µ—Å—Ç–∞
    companies_to_process = companies_df.head(limit).copy()

    print(f"üìä –ë—É–¥—É—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –ø–µ—Ä–≤—ã–µ {len(companies_to_process)} –∫–æ–º–ø–∞–Ω–∏–π")
    print("‚ÑπÔ∏è  –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 2-3 –º–∏–Ω—É—Ç—ã...")

    for idx, company in companies_to_process.iterrows():
        enriched_company = analyze_company_website(company.to_dict())
        results.append(enriched_company)

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        if (idx + 1) % 5 == 0:
            print(f"\nüìà –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {idx + 1}/{len(companies_to_process)}")

    # –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    enriched_df = pd.DataFrame(results)

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\n{'=' * 60}")
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê:")
    print(f"{'=' * 60}")

    stats = {
        '–í—Å–µ–≥–æ –∫–æ–º–ø–∞–Ω–∏–π': len(enriched_df),
        '–° –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π 24/7': enriched_df['mentions_24_7'].sum() if 'mentions_24_7' in enriched_df.columns else 0,
        '–° email –ø–æ–¥–¥–µ—Ä–∂–∫–∏': enriched_df[
            'has_support_email'].sum() if 'has_support_email' in enriched_df.columns else 0,
        '–° –∫–æ–Ω—Ç–∞–∫—Ç–Ω–æ–π —Ñ–æ—Ä–º–æ–π': enriched_df[
            'has_contact_form'].sum() if 'has_contact_form' in enriched_df.columns else 0,
        '–° –æ–Ω–ª–∞–π–Ω-—á–∞—Ç–æ–º': enriched_df['has_online_chat'].sum() if 'has_online_chat' in enriched_df.columns else 0,
        '–° —Ä–∞–∑–¥–µ–ª–æ–º –ø–æ–¥–¥–µ—Ä–∂–∫–∏': enriched_df[
            'has_support_section'].sum() if 'has_support_section' in enriched_df.columns else 0,
        '–° FAQ/–ë–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π': enriched_df['has_kb_or_faq'].sum() if 'has_kb_or_faq' in enriched_df.columns else 0,
        '–° –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ–º 10+': (enriched_df[
                                      'support_team_size_min'] >= 10).sum() if 'support_team_size_min' in enriched_df.columns else 0
    }

    for key, value in stats.items():
        print(f"   {key:25}: {value:3d}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    save_enriched_data(enriched_df)

    return enriched_df


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–∞–π—Ç–æ–≤"""
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–º–ø–∞–Ω–∏–∏
        companies_df = load_companies_from_csv()

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–æ–º–ø–∞–Ω–∏–∏
        enriched_df = process_companies(companies_df, limit=20)

        print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–æ–º–ø–∞–Ω–∏–π: {len(enriched_df)}")

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –¥–∞–ª—å–Ω–µ–π—à–∏–º —à–∞–≥–∞–º
        if 'support_team_size_min' in enriched_df.columns:
            companies_with_evidence = enriched_df[enriched_df['support_team_size_min'] >= 10]

            if len(companies_with_evidence) > 0:
                print(f"\nüéØ –ö–æ–º–ø–∞–Ω–∏–∏ —Å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞–º–∏ 10+ —á–µ–ª–æ–≤–µ–∫:")
                for idx, company in companies_with_evidence.head(10).iterrows():
                    evidence = company.get('support_evidence', '')[:50]
                    print(f"   ‚Ä¢ {company['name']}: {company['support_team_size_min']} —á–µ–ª. - {evidence}...")
            else:
                print("\n‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∫–æ–º–ø–∞–Ω–∏–π —Å –ø—Ä—è–º—ã–º–∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞–º–∏ 10+")
                print("   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–∞–∑–¥–µ–ª—ã '–ö–∞—Ä—å–µ—Ä–∞' –∏–ª–∏ '–í–∞–∫–∞–Ω—Å–∏–∏' –Ω–∞ —Å–∞–π—Ç–∞—Ö")

        return enriched_df

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()





